{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torchvision.models import resnet18, ResNet18_Weights\n",
    "from Dataloader import WholeDataset,BaseModel,ResetWeigtsDataset\n",
    "from torch.utils.data import  DataLoader,SubsetRandomSampler\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from Config import config,Args\n",
    "from Metrics import ComputeMetrics\n",
    "from sklearn.model_selection import KFold\n",
    "from tqdm import tqdm\n",
    "from models.DimRed import zca_whitening_matrix\n",
    "import math\n",
    "from sklearn.neighbors import KernelDensity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "args_train = Args(\"train\")\n",
    "train_dataset = WholeDataset(args_train)\n",
    "args_test  = Args(\"test\")\n",
    "test_dataset = WholeDataset(args_test)\n",
    "\n",
    "train_dataset = ResetWeigtsDataset(\"ResnetFeaturesTrain.pt\",\"train\")\n",
    "test_dataset = ResetWeigtsDataset(\"ResnetFeaturesTest.pt\",\"test\")\n",
    "\n",
    "train_dataloader = DataLoader(train_dataset,shuffle=True,batch_size=40000,num_workers=4,pin_memory=True,persistent_workers=True)\n",
    "test_dataloader = DataLoader(test_dataset,shuffle=False,batch_size=2000,num_workers=4,pin_memory=True,persistent_workers=True)\n",
    "X_t,y_t,s_t = next(iter(train_dataloader))#train_dataset[:1000]\n",
    "X_v,y_v,s_v = next(iter(test_dataloader))#test_dataset[:1000] \n",
    "#X_t= X_t.flatten(start_dim=1)\n",
    "#X_v =  X_v.flatten(start_dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "pic should be 2/3 dimensional. Got 4 dimensions.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_1837000/784196579.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtrain_dataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/research/hal-gaudisac/project802/Dataloader.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, index)\u001b[0m\n\u001b[1;32m    100\u001b[0m         \u001b[0mimage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimage\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    101\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 102\u001b[0;31m         \u001b[0mimage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mToPIL\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    103\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    104\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/research/hal-gaudisac/anaconda3/lib/python3.9/site-packages/torchvision/transforms/transforms.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, img)\u001b[0m\n\u001b[1;32m     93\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__call__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mimg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     94\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mt\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransforms\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 95\u001b[0;31m             \u001b[0mimg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     96\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mimg\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     97\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/research/hal-gaudisac/anaconda3/lib/python3.9/site-packages/torchvision/transforms/transforms.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, pic)\u001b[0m\n\u001b[1;32m    225\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    226\u001b[0m         \"\"\"\n\u001b[0;32m--> 227\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_pil_image\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpic\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    228\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    229\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__repr__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/research/hal-gaudisac/anaconda3/lib/python3.9/site-packages/torchvision/transforms/functional.py\u001b[0m in \u001b[0;36mto_pil_image\u001b[0;34m(pic, mode)\u001b[0m\n\u001b[1;32m    276\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpic\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndarray\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    277\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mpic\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndim\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m3\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 278\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"pic should be 2/3 dimensional. Got {pic.ndim} dimensions.\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    279\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    280\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0mpic\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndim\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: pic should be 2/3 dimensional. Got 4 dimensions."
     ]
    }
   ],
   "source": [
    "train_dataset[[1,2]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "KNeighborsClassifier()"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "knn = KNeighborsClassifier(n_neighbors=5)\n",
    "knn.fit(X_t,y_t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/research/hal-gaudisac/anaconda3/lib/python3.9/site-packages/sklearn/neighbors/_classification.py:228: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  mode, _ = stats.mode(_y[neigh_ind, k], axis=1)\n"
     ]
    }
   ],
   "source": [
    "y_pred = knn.predict(X_v) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "({'f1': tensor([0.5921, 0.6313, 0.2920, 0.3556, 0.3514, 0.3308, 0.2200, 0.3140, 0.3103,\n",
       "          0.2613]),\n",
       "  'accuracy': tensor([0.5294, 0.8492, 0.1724, 0.2991, 0.2364, 0.2529, 0.1264, 0.1919, 0.2022,\n",
       "          0.7979]),\n",
       "  'precesion': tensor([0.6716, 0.5023, 0.9524, 0.4384, 0.6842, 0.4783, 0.8462, 0.8636, 0.6667,\n",
       "          0.1562]),\n",
       "  'recall': tensor([0.5294, 0.8492, 0.1724, 0.2991, 0.2364, 0.2529, 0.1264, 0.1919, 0.2022,\n",
       "          0.7979]),\n",
       "  'confmat': tensor([[ 45,   0,   0,  12,   0,   2,   0,   0,   0,  26],\n",
       "          [  0, 107,   0,   0,   0,   0,   0,   0,   0,  19],\n",
       "          [  7,  32,  20,   7,   0,   2,   0,   1,   0,  47],\n",
       "          [  0,  12,   1,  32,   0,  10,   0,   0,   4,  48],\n",
       "          [  0,   8,   0,   1,  26,   0,   1,   0,   0,  74],\n",
       "          [  4,   5,   0,   4,   0,  22,   0,   0,   5,  47],\n",
       "          [ 10,   8,   0,   0,   4,   7,  11,   0,   0,  47],\n",
       "          [  0,  23,   0,   0,   1,   0,   0,  19,   0,  56],\n",
       "          [  1,  10,   0,  14,   1,   3,   1,   0,  18,  41],\n",
       "          [  0,   8,   0,   3,   6,   0,   0,   2,   0,  75]])},\n",
       " {'f1': tensor(0.3659),\n",
       "  'accuracy': tensor(0.3658),\n",
       "  'precesion': tensor(0.6260),\n",
       "  'recall': tensor(0.3658)})"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "compute_metrics(torch.tensor(y_pred),y_v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from Dataloader import ClassConditionalPermutation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "classCondPerm = ClassConditionalPermutation(0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/research/hal-gaudisac/project802/Dataloader.py:107: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at /opt/conda/conda-bld/pytorch_1674893616450/work/torch/csrc/utils/tensor_numpy.cpp:206.)\n",
      "  label_image = torch.from_numpy(np.transpose(label_image,(2,0,1)))\n",
      "/research/hal-gaudisac/project802/Dataloader.py:107: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at /opt/conda/conda-bld/pytorch_1674893616450/work/torch/csrc/utils/tensor_numpy.cpp:206.)\n",
      "  label_image = torch.from_numpy(np.transpose(label_image,(2,0,1)))\n",
      "/research/hal-gaudisac/project802/Dataloader.py:107: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at /opt/conda/conda-bld/pytorch_1674893616450/work/torch/csrc/utils/tensor_numpy.cpp:206.)\n",
      "  label_image = torch.from_numpy(np.transpose(label_image,(2,0,1)))\n",
      "/research/hal-gaudisac/project802/Dataloader.py:107: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at /opt/conda/conda-bld/pytorch_1674893616450/work/torch/csrc/utils/tensor_numpy.cpp:206.)\n",
      "  label_image = torch.from_numpy(np.transpose(label_image,(2,0,1)))\n",
      "/research/hal-gaudisac/project802/Dataloader.py:107: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at /opt/conda/conda-bld/pytorch_1674893616450/work/torch/csrc/utils/tensor_numpy.cpp:206.)\n",
      "  label_image = torch.from_numpy(np.transpose(label_image,(2,0,1)))\n",
      "/research/hal-gaudisac/project802/Dataloader.py:107: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at /opt/conda/conda-bld/pytorch_1674893616450/work/torch/csrc/utils/tensor_numpy.cpp:206.)\n",
      "  label_image = torch.from_numpy(np.transpose(label_image,(2,0,1)))\n",
      "/research/hal-gaudisac/project802/Dataloader.py:107: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at /opt/conda/conda-bld/pytorch_1674893616450/work/torch/csrc/utils/tensor_numpy.cpp:206.)\n",
      "  label_image = torch.from_numpy(np.transpose(label_image,(2,0,1)))\n",
      "/research/hal-gaudisac/project802/Dataloader.py:107: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at /opt/conda/conda-bld/pytorch_1674893616450/work/torch/csrc/utils/tensor_numpy.cpp:206.)\n",
      "  label_image = torch.from_numpy(np.transpose(label_image,(2,0,1)))\n"
     ]
    }
   ],
   "source": [
    "train_dataloader = DataLoader(train_dataset,shuffle=True,batch_size=1000,num_workers=4,pin_memory=True,persistent_workers=True)\n",
    "test_dataloader = DataLoader(test_dataset,shuffle=False,batch_size=1000,num_workers=4,pin_memory=True,persistent_workers=True)\n",
    "X_t,l_t,y_t,s_t = next(iter(train_dataloader))#train_dataset[:1000]\n",
    "X_v,l_v,y_v,s_v = next(iter(test_dataloader))#test_dataset[:1000] \n",
    "U = classCondPerm(X_t,y_t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 3 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "fig,ax = plt.subplots(1,3)\n",
    "ax[0].imshow(np.transpose((U[y_t==8] )[1],(1,2,0)))\n",
    "ax[1].imshow(np.transpose((U[y_t==0] )[1],(1,2,0)))\n",
    "ax[2].imshow(np.transpose((U[y_t==2] )[1],(1,2,0)))\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Pipeline(steps=[('standardscaler', StandardScaler()),\n",
       "                ('svc', SVC(gamma='auto'))])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.svm import SVC\n",
    "from Metrics import ComputeMetrics\n",
    "compute_metrics =ComputeMetrics(10)\n",
    "clf = make_pipeline(StandardScaler(), SVC(gamma='auto'))\n",
    "clf.fit(X_t, y_t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "l=[]\n",
    "for X_v,y_v,s_v in iter(test_dataloader):\n",
    "    Xv_flat  = X_v.flatten(start_dim=1)\n",
    "    y_pred = clf.predict(Xv_flat)\n",
    "    metrics = compute_metrics(torch.tensor(y_pred),y_v)\n",
    "    l.append(metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.41508861184120177, 0.0077237196693616325)"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "u =[]\n",
    "for metric in l:\n",
    "    u.append(metric[1][\"recall\"].item())\n",
    "u = np.array(u)\n",
    "u.mean(),np.sqrt(u.var())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "l=[]\n",
    "for X_v,y_v,s_v in iter(train_dataloader):\n",
    "    Xv_flat  = X_v.flatten(start_dim=1)\n",
    "    y_pred = clf.predict(Xv_flat)\n",
    "    metrics = compute_metrics(torch.tensor(y_pred),y_v)\n",
    "    l.append(metrics)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.8958479166030884, 0.0)"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the the current cell or a previous cell. Please review the code in the cell(s) to identify a possible cause of the failure. Click <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. View Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "u =[]\n",
    "for metric in l:\n",
    "    u.append(metric[1][\"accuracy\"].item())\n",
    "u = np.array(u)\n",
    "u.mean(),np.sqrt(u.var())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/research/hal-gaudisac/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "clf = LogisticRegression(random_state=0).fit(X_t, y_t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
